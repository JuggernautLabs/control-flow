We are working on the framework framework (f(fw)w)

to start we will be testing to see what kinds of tools we should build to abstract on top of the strongly typed llm interface in ../client-implementations

my reasoning tells me that we will begin toying with human-ai knowledge extrapolation (called tlamatini) and in doing so we will be looking into epistomoligical and statistical foundations like 

1.  how can we have confidence that generated facts (true or false) represent the underlying structure of the representation; ie. "the file doesn't exist" is true and reflects the non-existence of a file, "the code is tested and passes" is true and represents that the tests reflect the code, and the code reflects the spec, and the tests have been executed and return a passing score; "the code doesn't have vulnerabilities" is false and reflects that the code has an issue and more work must be done to eliminate it

2. how can we have confidence in the semantic meaning of generated facts? "generate qa pairs for this text" requires us to generate pairs of strings, and then verify that those strings represent qa pairs which reflect the text; this can be observed through recursive decomposition, where each pair is related to the text and verified -- but how can a system have confidence that any single pair isn't being hallucinated? we can utilize delegations of humans to manually view the generated data and provide their feedback on the semantic meaning and it's relation to the text; it may be possible to utilize delegations of cognitive systems (of a certain level of quality) to perform those verification steps

3. how can we construct knowledge verification chains which allow us to have confidence in high level facts like "the qa webapp works as expected?" this requires us to have definitions for expected behavior and systems for observing and verifying expected behavior ie. down one chain: webapp works -> backend works -> qa endpoint can generate new pairs given a document id -> a pair can be reliably generated on text given a particular cognitive agent and prompted through query_with_schema -> the claude cognitive agent can reliably return structured data -> integration tests pass. at any one of these steps, it can be observed that the verification chain has multiple branches. The construction of the verification lattice will be an important aspect of any reliable system